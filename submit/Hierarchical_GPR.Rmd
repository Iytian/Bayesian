---
title: "Hierarchical Gaussian Process Model"
author: "Ian(Yuan) Tian"
date: "`r Sys.Date()`"
biblio-style: apalike
link-citations: true
bibliography: references_HGPR.bib
output: 
  html_document:
    number_sections: false
    toc: true
    toc_float: false

---

\newcommand{\bm}[1]{\boldsymbol{#1}}
\newcommand{\yy}{{\bm y}}
\newcommand{\xx}{{\bm x}}
\newcommand{\zz}{{\bm z}}
\newcommand{\YY}{{\bm Y}}
\newcommand{\XX}{{\bm X}}
\newcommand{\UU}{{\bm U}}
\newcommand{\KK}{{\bm K}}
\newcommand{\bb}{{\bm \beta}}


## Gaussian Process Regression

In the Gaussian Process section, we have discussed the basic idea of Gaussian Process Regression model, which the target variable $\yy$ is the combination of Gaussian process function $f(\XX)$ with a noise term $\epsilon$. So teh model can be written as:
$$
\yy = f(\XX) + \epsilon,
$$
and $y|f(\xx) \sim \mathcal{N}\left(f(\xx), \sigma^2 \right)$, $f(\xx) \sim GP\left(\xx^{T}\bb, \sigma^2 \KK_{\theta}(\xx, \xx^{\star}) \right)$.

By combining the training and test data, the joint distribution can be written as:
$$
\left(\begin{matrix}
\yy \\ f(\XX)_\star 
\end{matrix}\right)
\sim \mathcal{N}\left(
\left(\begin{matrix}
\XX\bb \\ \XX_{\star}\bb
\end{matrix}\right),
\sigma^2
\left( \begin{matrix} 
\KK + \bm{I} & \KK_\star\\
\KK_\star^T & \KK_{\star\star}\\
\end{matrix} \right)\right),
$$
for the prediction, the conditional distributon for the test data can be written as:
\begin{equation}
f(\XX)_{\star}|\yy, \XX, \bm{\Theta} \sim \mathcal{N}\left( \bm{\mu}_{\star}, \sigma^2\bm{\Sigma}_{\theta} \right),
\label{eq:conddist}
\end{equation}

where 
$$
\begin{align*}
\bm{\mu}_{\star} &= \XX_{\star}\bb + \KK_\star^T(\bm{I} + \KK)^{-1}(\yy - \XX\bb), \\
\bm{\Sigma}_{\theta} &= \KK_{\star\star} - \KK_\star^T(\bm{I} + \KK)^{-1}\KK_\star.
\end{align*}
$$
In addition to this, we can further assume that the parameters in the Gaussian Process Regression are random, and following certain distributions. This Hierarchical model setup will be illustrated in the following section.

##  Hierarchical Gaussian Process Regression

In this section, we are going to using Multivariate Normal-Inverse-Gamma as the conditional conjugate prior for Gaussian Process Regression model, and the distribution of these parameters can be defined as:
$$
\begin{align*}
\bm{\theta} &\sim \pi(\bm{\theta}), \\
\sigma^2|\bm{\theta} &\sim \text{Inv-Gamma}(0.5\nu, 0.5\nu\tau),\\
\bm{\beta}|\sigma^2,\bm{\theta} &\sim \mathcal{N}(\bm{\lambda}, \sigma^2\bm{\Omega}),
\end{align*}
$$
so $\bm{\theta}$, in this case, represents the set of width hyperparameter and length hyperparameter for the radial kernel; then the joint posterior is a multivariate Normal-Inv-Gamma distribution, and it is define as:
\begin{equation}
(\bm{\beta}, \sigma^2|\bm{\theta},\YY) \sim mNIG(\hat{\bm{\lambda}}, \hat{\bm{\Omega}}, \hat{\nu}, \hat{\tau}),
\label{eq:jointpost}
\end{equation}

where,
$$
\begin{align*}
\hat{\bm{\Omega}} &= \XX^{T}\bm{V}_\theta\XX + \bm{\Omega}, \ \ \ &&\hat{\nu} = N + \nu, \\
\hat{\bm{\lambda}} &= \hat{\bm{\Omega}}^{-1}(\bm{\Omega}\bm{\lambda} + \XX^{T}\bm{V}_\theta\yy), \ \ \ 
&&\hat{\tau} = \hat{\nu}^{-1}\left(\yy^{T}\bm{V}_\theta\yy -  \hat{\bm{\lambda}}^{T}\hat{\bm{\Omega}}\hat{\bm{\lambda}} +
\bm{\lambda}^{T}\bm{\Omega}\bm{\lambda} + \nu\tau \right), 
\end{align*}
$$
and $\bm{V}_\theta = \left(\bm{I} + \KK_\theta(\XX,\XX) \right)^{-1}$.

So the log of marginal posterior can be written out explicitly as:
\begin{equation}
p(\bm{\theta}, \bm{\psi}|\YY,\XX) = \pi(\bm{\theta},\bm{\psi}) - \zeta(\bm{\psi}) + \zeta(\hat{\bm{\psi}}) - \frac{log|\bm{V}|}{2} - \frac{Nlog(2\pi)}{2},
\label{eq:marginpost}
\end{equation}


where 
$$
\begin{align*}
\zeta(\bm{\psi}) &= 0.5\left[ 2log\Gamma(0.5\nu) - log|\bm{\Omega}| - {\nu}log(0.5\nu\tau) \right], \\
\bm{\phi} &= (\bm{\lambda}, \bm{\omega}, log\nu, \log\tau).
\end{align*}
$$
And based on the suggestion from the vinette of `losmix`, we assume a prior distribution on log of Cholesky factor of $\bm{\Omega}$ in order to keep the invariant to permutation of the elements of $\bm{\Omega}$,
$$
\text{i.e. } \pi(\bm{\theta},\bm{\psi}) \propto \prod_{j = 1}^{p}\exp(\omega_j)^{p-j},
$$
$p$ is the dimention of $\bm{\lambda}$, and this completes the model setup of Hierarchical Gaussian Process Regression.

### Bayesian Inference

The inference of this Hierarchical model is performed by using simulation study in the following three setps:

1.  Simulates the values of hyperparameters from the marginal posterior distribution in Equation \\eqref{eq:marginpost}.
2.  Given the values of hyperparameters, then simulated the parameter values from the joint posterior distribution in Equation \\eqref{eq:jointpost}.
3.  Then the predictions for each simulations can be generated by using the the condition distbution in \\eqref{eq:conddist}.

So, after performing the three steps, the predictions are sampled from:
$$
p(f(\XX)_\star|\YY,\XX) = {\int}p(f(\XX)_\star|\YY,\XX,\bm{\beta},\sigma^2)p(\bm{\beta},\sigma^2|\bm{\theta},\bm{\psi})p(\bm{\theta}, \bm{\psi}|\YY,\XX)d(\bm{\beta},\sigma^2,\bm{\theta},\bm{\psi}).
$$

The implementaion of these three steps are combined R and C++ which is trying to optimize the computation efficiency, and `losmix` has most of the works done; so we just need to modified some of the functions in C++.  

### Implementation

####  Simulation 

At the begining, we need to simulate some dataset from Gaussian Process Regression based on the assumed distributions disscused above.
First, we are going to load the required packages`losmix`, `TMB`, `MASS`, `optimCheck` for this section, and reading the local functions, then, the simulation is performed as following,
```{r loadingpackages, include = F}
## loading the packages and functions
require(losmix)
require(TMB)
require(MASS)
require(optimCheck)
```

```{r simulation}

source("solveV.R")
source("Hierarchical_GPR_functions.R")
compile("ModelExt_GPR.cpp")
dyn.load(dynlib("ModelExt_GPR"))
compile("ModelExt_GPR_forced.cpp")
dyn.load(dynlib("ModelExt_GPR_forced"))
##  simulation

# defining the hyperparameters
p <- 2 #  beta1, beta2
nsamples <- 50 #  number of training data

set.seed(10)
# hyperparameters for betas
lambda <- rnorm(p)
Omega <- crossprod(matrix(rnorm(4), 2, 2))  # precision matrix
logc_Omega <- chol(Omega) # using log of Cholesky factors
diag(logc_Omega) <- log(diag(logc_Omega)) 
logc_Omega <- logc_Omega[upper.tri(logc_Omega, diag = TRUE)]

# hyperparameters for sigma
nu <- 5 * runif(1) + 3
tau <- rexp(1) / 100

# generates sigma and beta
sigma <- 1 / sqrt(rgamma(n = 1, shape = 0.5 * nu, rate = 0.5 * nu * tau))
beta <- mvrnorm(1, lambda, sigma^2 * solve(Omega))

# hyperparameters for kernel matrix
k_w <- runif(1, 0.5, 1.5) / sigma
k_l <- runif(1, 1.5, 2.5)

# generates the design matrix
x <- matrix(rnorm(nsamples * p), ncol = p, nrow = nsamples)

f_x <- x %*% beta # mean for response variable = y

# L2 distance matrix
dist2 <- as.matrix(dist(x))^2
  
# covariance matrix for response variable - y
cov_mat <- diag(rep(1, nsamples)) + kernel(x, w = k_w, l = k_l)

# simulated response variable
y_true <- as.matrix(mvrnorm(1, f_x, sigma^2 * cov_mat))


hyperparameters <- c(lambda, logc_Omega[1], logc_Omega[2], logc_Omega[3], log(nu), log(tau), log(k_w), log(k_l))

```


####  Marginal Posterior Distribution

As the discussion above in step 1 of Bayesian Inference, we need to simulate from the marginal posterior distribution. This can be achieved by using either MCMC or using normal approximation to generate the hyperparameters. For the first method, the Metropolis-Within-Gibbs alorithm is coded in the auxiliary functions; however, for this large numbers of hyperparameters, this alogrithm is very unstable, which might require a more advanced alogrithm to simulated these hyperparameters.
```{r MCMC}
# Metropolis-Within-Gibbs
# for illustration porpuse, only 5 samples were simulated
mwg_out <- hyperparameter_MWG(n = 5, y = y_true, xx = x, theta = hyperparameters, mcmc_std = 1)
print(mwg_out)

# true values
print(hyperparameters)
```

Therefore, the normal approximation is used to simulate the hyperparameters, and, in order to make it more efficient, this method is coded in C++, and the functions are called by using `TMB` package in R. To perform the normal approximation, the mean value is estimated by using MAP method, which estimated the hyperparameters by maximizing the marginal posterial distribution; on the other hand, the covariance matrix is estimated using the inverse of the observed information:
$$
\text{i.e. } (\hat{\bm{\psi}},\hat{\bm{\theta}}) = \text{argmin}_{\bm{\psi},\bm{\theta}}p(\bm{\psi},\bm{\theta}|\YY,\XX), \ \ \
O(\bm{\psi},\bm{\theta}|\YY,\XX) = -\frac{\partial^2}{\partial (\bm{\psi},\bm{\theta})^2}p(\bm{\psi},\bm{\theta}|\YY,\XX).
$$

Before we simulating the hyperparameters by using the normal approximation, we need to make sure that the function we implemented is correct, and the way to validate this is by using the the equation on different training data that [log-likelihoood function] + [log-prior] = [log-marginal posterior] + [log-joint posterior]:
$$
logP(\YY|\XX,\bm{\beta},\sigma^2,\bm{\theta},\bm{\psi}) + logP(\bm{\beta},\sigma^2,\bm{\theta},\bm{\psi}) = 
logP(\bm{\theta},\bm{\psi}|\YY,\XX) + logP(\bm{\beta},\sigma^2|\YY,\XX,\bm{\theta},\bm{\psi}).
$$
And the code below is going to validate the marginal posterior distribution we implemented is corret.
```{r margpostvalidation}

##  function to simluate the training dataset - this part used losmix for the other three log probabilities
simulation_validation <- function() {
  # assumed p = 2
  
  # hyperparameters for betas  
  lambda0 <- rnorm(2)
  Omega0 <- crossprod(matrix(rnorm(4), 2, 2))
  logc_Omega0 <- chol(Omega0)
  diag(logc_Omega0) <- log(diag(logc_Omega0)) 
  logc_Omega0 <- logc_Omega0[upper.tri(logc_Omega0, diag = TRUE)]
  
  # hyperparameters for sigma and kernel
  nu0 <- runif(1, 1, 2);  tau0 <- rexp(1)/ 5;  k_w0 <- runif(1, 1, 5);  k_l0 <- runif(1, 1, 5)
  
  sigma0 <- 1 / sqrt(rgamma(n = 1, shape = 0.5 * nu0, rate = 0.5 * (nu0 * tau0)))
  beta0 <- mvrnorm(1, lambda0, sigma0^2 * solve(Omega0))
  
  x0 <- matrix(rnorm(10 * 2), ncol = 2, nrow = 10)
  
  f_x0 <- x0 %*% beta0 # mean for response variable - y
  
  # L2 distance matrix
  dist20 <- as.matrix(dist(x0))^2
  
  # covariance matrix for response variable - y
  cov_mat0 <- diag(rep(1, 10)) + kernel(x0, w = k_w0, l = k_l0)
  
  # simulated response variable
  y0 <- as.matrix(mvrnorm(1, f_x0, sigma0^2 * cov_mat0))
  
  # output list training dataset
  list(lambda = lambda0, Omega = Omega0, logc_Omega = logc_Omega0, nu = nu0, 
       tau = tau0, k_w = k_w0, k_l = k_l0, sigma = sigma0, beta = beta0, x = x0, 
       cov_mat = cov_mat0, y_true = y0, f_x = f_x0, dist2 = dist20)
}
##  comparison with 20 simulated training dataset
replicate(20, expr = {
  theta_t <- simulation_validation()
  
  #[log-prior]
  log_prior <- losmix::dsichisq(theta_t$sigma^2, nu = theta_t$nu, tau = theta_t$tau, log = TRUE) + 
    losmix::dmvn(theta_t$beta, mu = theta_t$lambda, Sigma = theta_t$sigma^2 * solve(theta_t$Omega), log = T) +
    lchol_prior(theta_t$Omega)
  
  #[log-likelihoood function]
  log_lik <- losmix::dmvn(as.vector(theta_t$y_true), as.vector(theta_t$f_x), theta_t$sigma^2 * theta_t$cov_mat, log = TRUE)
  
  #hyperparameters for the joint posterior distribution
  hat_values <- hat_estimators(theta_t)
  
  #[log-marginal posterior] - a constant scaling part was removed from the marginal posterior distribution
  #log_marg <- log_marginal_post(theta_t$y_true, theta_t$x, ttt) - 10/2 * log(2*pi)  # this is R implementation
  GPR_test <- MakeADFun(data = list(y = theta_t$y_true, Xtr = t(theta_t$x), dist2 = theta_t$dist2), 
                        parameters = list(lambda = theta_t$lambda, logC_Omega = theta_t$logc_Omega, 
                                          log_nu = log(theta_t$nu), log_tau = log(theta_t$tau), 
                                          log_ww = log(theta_t$k_w), log_ll = log(theta_t$k_l)), DLL = "ModelExt_GPR")
  log_marg <- -GPR_test$fn() - 10/2 * log(2*pi)  
  
  #[log-joint posterior]
  log_joint <- losmix::dsichisq(theta_t$sigma^2, nu = hat_values$nu_star, tau = hat_values$tau_star, log = TRUE) + 
    losmix::dmvn(theta_t$beta, mu = hat_values$lambda_star, Sigma = theta_t$sigma^2 * solve(hat_values$Omega_star), log = T)
  
  # the value of lhs should equal to value of rhs
  list(lhs = as.numeric(log_prior + log_lik), rhs = as.numeric(log_marg + log_joint))
})

```
As we can see that, the value on left hand side of the equaiton is euqal to the value on the right hand side of the euqation, which indicates that our implementation of the marginal posterior distribution is correct.

Next, we can use this marginal posterior distribution to simulate the hyperparameters by using normal approximation, and, first of all, we need to find the parameters for the normal distribution.
```{r MAP1, results="hide"}
## Maximizing a posterior

# call the compiled function
GPR_TMB <- MakeADFun(data = list(y = y_true, Xtr = t(x), dist2 = dist2),
                      parameters = list(lambda = lambda, logC_Omega = logc_Omega, 
                                        log_nu = log(nu), log_tau = log(tau),
                                        log_ww = log(k_w), log_ll = log(k_l)), DLL = "ModelExt_GPR")

# using nlminb, since it can assign contraint to the hyperparameters
marginal_post_MAP <- nlminb(GPR_TMB$par, GPR_TMB$fn, GPR_TMB$gr, lower = rep(-10,9), upper = rep(10,9))
```

```{r MAP-plot1, fig.cap = "MAP with all hyperparameters", fig.align = 'center'}
# plot negative log-likelihodd to see if it is at a local minimum
marginal_post <- optim_proj(fun = GPR_TMB$fn, # objective function
                                  xsol = marginal_post_MAP$par, 
                                  maximize = FALSE,
                                  xnames = c("lambda1", "lambda2","Omega1","Omega2","Omega3","nu","tau","ww","ll"))
```

The plot above show that some of the hyperparameters are very hard to reach a local minimum value, but, in order to continue on the example, let forced these volatile ones to be the true value (this is because we know the true value..., need to find a better way to solve this problem). And let's re-do the MAP,
```{r MAP2, results="hide"}
## Maximizing a posterior again...

# call the compiled function
GPR_TMB_forced <- MakeADFun(data = list(y = y_true, Xtr = t(x), dist2 = dist2, 
                                        logC_Omega = logc_Omega, log_nu = log(nu)),
                      parameters = list(lambda = lambda, log_tau = log(tau),
                                        log_ww = log(k_w), log_ll = log(k_l)), DLL = "ModelExt_GPR_forced")

# after forcing the hyperparameters we can use nlm to perform MAP.
# it is actually minimizing negative log-likelihood which is the same.
ofun <- function(par) {
  out <- GPR_TMB_forced$fn(par)
  # include gradient information via 'attribute'
  attr(out, "gradient") <- GPR_TMB_forced$gr()
  out
}
GPR_TMB_forced$fn()
# optimization
opt <- nlm(p = GPR_TMB_forced$par, # starting value (losmix picks a reasonable default)
           f = ofun) # objective function
opt$code # code == 1 means that 'nlm' thinks it converged
```

```{r MAP-plot2, fig.cap = "MAP with selected hyperparameters", fig.align = 'center'}
# plot negative log-likelihodd to see if it is at a local minimum
marginal_post1 <- optim_proj(fun = GPR_TMB_forced$fn, # objective function
                                  xsol = opt$estimate, 
                                  maximize = FALSE,
                                  xnames = c("lambda1","lambda2","tau","ww","ll"))

```

We can see that all the hyperparameters are at the local minimum, then we can use these estimated values to simulated the hyperparameters using normal approximation.
```{r simulation1}
##  There is an error in solving the hessian matrix...
# npost <- 10^4
# normal_approx_mean <- opt$estimate # (approximate) posterior mean
# normal_approx_var <- solveV(GPR_TMB_forced$he(opt$estimate)) # (approximate) posterior variance
# simulated_hyperparameters <- rmvn(n = npost, mu = normal_approx_mean, Sigma = normal_approx_var)

```

```{r simulation2}
# alternatvely the auxiliary functions provided percentile function implemented in R
# this function can be used if using MCMC method.
test_simulation <- rmNIG(n = 3000, lambda = lambda, Omega = Omega, a = 0.5 * nu, b = 0.5 * nu * tau)
theta_true <- c(beta, sigma)
theta_names <- c("beta[1]", "beta[2]", "sigma")
par(mfrow = c(1,3), mar = c(2,2,4,.5))
for(ii in 1:3) {
  # approximate posterior
  hist(test_simulation[,ii], breaks = 40, xlab = "", ylab = "",
       main = parse(text = paste0("hat(p)(", theta_names[ii],
                                  "*\" | \"*bold(Y),bold(X))")))
  # true parameter value
  abline(v = theta_true[ii], col = "red", lwd = 2)
}

```

### Prediction
